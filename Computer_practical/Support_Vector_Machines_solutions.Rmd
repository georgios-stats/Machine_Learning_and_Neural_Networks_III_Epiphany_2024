---
title: "Support Vector Machines (SVM)"
subtitle: "Binary classification"
author: "Georgios P. Karagiannis @ MATH3431 Machine Learning and Neural Networks III"
output:
  html_notebook: 
    number_sections: true
  word_document: default
  html_document:
    df_print: paged
    number_sections: true
  pdf_document: default
header-includes: 
  - \usepackage{tikz}
  - \usepackage{pgfplots}
  - \usepackage{amsmath}
---

<!-- -------------------------------------------------------------------------------- -->

<!-- Copyright 2023 Georgios Karagiannis -->

<!-- georgios.karagiannis@durham.ac.uk -->
<!-- Associate Professor -->
<!-- Department of Mathematical Sciences, Durham University, Durham,  UK  -->

<!-- This file is part of Machine Learning and Neural Networks III (MATH3431) -->
<!-- which is the material of the course (MATH3431 Machine Learning and Neural Networks III) -->
<!-- taught by Georgios P. Katagiannis in the Department of Mathematical Sciences   -->
<!-- in the University of Durham  in Michaelmas term in 2019 -->

<!-- Machine_Learning_and_Neural_Networks_III_Epiphany_2023 is free software: you can redistribute it and/or modify -->
<!-- it under the terms of the GNU General Public License as published by -->
<!-- the Free Software Foundation version 3 of the License. -->

<!-- Machine_Learning_and_Neural_Networks_III_Epiphany_2023 is distributed in the hope that it will be useful, -->
<!-- but WITHOUT ANY WARRANTY; without even the implied warranty of -->
<!-- MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the -->
<!-- GNU General Public License for more details. -->

<!-- You should have received a copy of the GNU General Public License -->
<!-- along with Machine_Learning_and_Neural_Networks_III_Epiphany_2023  If not, see <http://www.gnu.org/licenses/>. -->

<!-- -------------------------------------------------------------------------------- -->



[Back to README](https://github.com/georgios-stats/Machine_Learning_and_Neural_Networks_III_Epiphany_2024/tree/main/Computer_practical#aim)

```{r}
rm(list=ls())
```


---

***Aim***

Students will become able to:  

+ practice in R,  

+ implement Support Vector Machine (classifiers) with R package **e1071** in R.   

---

***Reading material***


+ Lecture notes:  
    + [Handout 7: Support Vector Machines](https://github.com/georgios-stats/Machine_Learning_and_Neural_Networks_III_Epiphany_2024/blob/main/Lecture_handouts/07.Support_Vector_Machines.pdf)  
    
+ References for the R package (optional supplementary material for your information)  
    + [e1071 in R Cran](https://cran.r-project.org/web/packages/e1071/)  
    + [e1071 Vignettes](https://cran.r-project.org/web/packages/e1071/vignettes/svmdoc.pdf)    
    + [e1071 Vignettes](https://cran.r-project.org/web/packages/e1071/vignettes/svminternals.pdf)    

+ Reference for *R*:  
    
    + [Cheat sheet with basic commands](https://cran.r-project.org/doc/contrib/Short-refcard.pdf)   

+ Reference of *rmarkdown* (optional, supplementary material):  
  
    + [R Markdown cheatsheet](https://raw.githubusercontent.com/rstudio/cheatsheets/main/rmarkdown-2.0.pdf)  

    + [R Markdown Reference Guide](https://www.rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf)  

    + [knitr options](https://yihui.name/knitr/options)

+ Reference for *Latex* (optional, supplementary material):  
  
    + [Latex Cheat Sheet](https://wch.github.io/latexsheet/latexsheet-a4.pdf)  

---

***New software*** 

+ R package **e1071** functions:    
    + **svm{e1071}** : Used to train SVM.  
    + **predict{e1071}**: Using this method, we obtain predictions from the model, as well as decision values from the binary classifiers.
    + **plot{e1071}** : Visualizing data, support vectors and decision boundaries, if provided.   
    + **tune{e1071}** : Hyperparameter tuning uses tune() to perform a grid search over specified parameter ranges.

---


```{r, results="hide"}
# Load R package for printing
library(knitr)
```

```{r}
# Set a seed of the randon number generator
set.seed(2023)
```

# Getting familiar with R package e1071 

We will use the R package e1071 which is available from  

+ [https://cran.r-project.org/web/packages/e1071/](https://cran.r-project.org/web/packages/e1071/)  

The reference manual is available from  

+ [https://cran.r-project.org/web/packages/e1071/e1071.pdf](https://cran.r-project.org/web/packages/e1071/e1071.pdf)  

Details

+ **svm{e1071}** : Used to train SVM.  

+ **predict{e1071}**: Using this method, we obtain predictions from the model, as well as decision values from the binary classifiers.

+ **plot{e1071}** : Visualizing data, support vectors and decision boundaries, if provided.   

+ **tune{e1071}** : Hyperparameter tuning uses tune() to perform a grid search over specified parameter ranges. 

## Install e1071 
 
```{r}
## build version (recommended)
#install.packages("e1071")
## linux build
#install.packages("https://cran.r-project.org/src/contrib/e1071_1.7-14.tar.gz", repos = NULL, type = "source")
## windows build
#install.packages("https://cran.r-project.org/bin/windows/contrib/4.4/e1071_1.7-14.zip", repos = NULL, type = "source")
```

## Load the R package e1071  

```{r}
library(e1071)
```

<!---
## About e1071 commands 

Check out R package **e1071** commands from the reference manual is available from  

+ [https://cran.r-project.org/web/packages/e1071/e1071.pdf](https://cran.r-project.org/web/packages/e1071/e1071.pdf) 

in particular:  **svm{e1071}**, **predict{e1071}**, **plot{e1071}**, **tune{e1071}**  

+ Library **e1071** contains implementations for a number of statistical learning methods. 

+ R function **svm()** can be used to fit a support vector classifier when the argument **kernel = "linear"** is used.  

+ **cost** argument allows us to specify the cost of a violation to the margin.  

  + It is the cost of constraints violation (default: 1) --it is the "C"-constant of the regularization term in the Lagrange formulation.; as follows.  
  \[
  \begin{align}
\left(w^{*},b^{*},\xi^{*}\right) & =\underset{\left(w,b,\xi\right)}{\arg\min}\left(\frac{1}{2} \left\Vert w\right\Vert _{2}^{2}+C\frac{1}{m}\sum_{i=1}^{m}\xi_{i}\right)\label{eq:-2-1}\\
\text{subject to: , ,}\, & y_{i}\left(\langle w^{*},x_{i}\rangle+b^{*}\right)\ge1-\xi_{i},\,\,\forall i=1,...,m\label{eq:-3-1}\\
 & \xi_{i}\ge0, , ,\forall i=1,...,m\label{eq:-5}
\end{align}
  \]

  + Note that $C=\frac{1}{2\lambda}$ where $\lambda$ is the regularization term in Algorithm Soft-SVM in Handout 8: Support Vector Machines.     

  + When the cost argument is small, then the margins will be wide and many support vectors will be on the margin or will violate the margin.  

  + When the cost argument is large, then the margins will be narrow and there will be few support vectors on the margin or violating the margin.  

Below we demonstrate the use of this function on a two-dimensional example so that we can plot the resulting decision boundary.  


-->

---

# Soft SVM   

<!--
Recall 

  \[
  \begin{align}
\left(w^{*},b^{*},\xi^{*}\right) & =\underset{\left(w,b,\xi\right)}{\arg\min}\left(\frac{1}{2} \left\Vert w\right\Vert _{2}^{2}+C\frac{1}{m}\sum_{i=1}^{m}\xi_{i}\right)\label{eq:-2-1}\\
\text{subject to: , ,}\, & y_{i}\left(\langle w^{*},x_{i}\rangle+b^{*}\right)\ge1-\xi_{i},\,\,\forall i=1,...,m\label{eq:-3-1}\\
 & \xi_{i}\ge0, , ,\forall i=1,...,m\label{eq:-5}
\end{align}
  \]
-->
  

## Load the data

Let's load the available training dataset in and then plot.  

```{r}
set.seed(1)
x <- matrix(rnorm(20 * 2), ncol = 2)
y <- c(rep(-1, 10), rep(1, 10))
x[y == 1, ] <- x[y == 1, ] + 1
```

In the plot below we see that they are not linearly separable, so a Soft SVM may be suitable to be used

```{r}
plot(x, col = (3 - y))
```


To use the functions of R package **e1071**, we need to code the straining dataset as a data.frame as follows 

```{r}
dat = data.frame(x, y = as.factor(y))
```



##   Fit the SVM classifier  

To fit the SVM classifier use the R function **svm{e1071}**

Check R Documentation of **svm{e1071}** with **?svm** before you use it.  

Here, use the arguments:  

+ **cost = 10** cost of constraints violation (default: 1)—it is the ‘C’-constant of the regularization term in the Lagrange formulation.

+ **kernel = "linear"** the kernel used in training and predicting; the linear is $k(v,u)=<v,u>$.   

+ **scale = FALSE** to impose svm not to scale each feature to have mean zero or standard deviation one. 

Save the output to an object with name **svmfit**

```{r}
svmfit <- svm(y ~ ., 
              data = dat, 
              kernel = "linear", 
              cost = 10, 
              scale = FALSE)
```



##  Print the summary of training 

Use the R function **summary()** to print the basic information summary of the training.  

Check in R documentation of "svm{e1071}"  

+ What type of svm did you actually use?  

+ What type of kernel of svm did you actually use? 

+ What is the numerical coding of the response?  

+ How many support vector did you use? 

```{r}
summary(svmfit)
```

##  Plot the support vector classifier  

Plot the support vector classifier obtained by using function **plot.svm {e1071}** .  

Use it as **plot( my_obj , my_data )**   

```{r}
plot(svmfit, dat)
```

In the plot:  

+ The region of feature space that will be assigned to the -1 class is shown in light yellow, and the region that will be assigned to the +1 class is shown in red.  

+ The points that are represented by an "X" are the support vectors, or the points that directly affect the classification line.  

+ The points marked with an "O" are the other points, which don’t affect the calculation of the line.  

+ The decision boundary between the two classes is linear (because of kernel = "linear")   


## Investigate of the output produced by **svm {e1071}**  

Have a look at the paragraph "Values" in the  R documentation of **svm {e1071}**.  


###  Get the support vectors

1. Print the resulting support vectors by using **svmfit$SV**, and double check with the previous plot.    

2. Print the index of the resulting support vectors in the data matrix by using **svmfit$index**.  

```{r}
# 1
print(svmfit$SV)
```
```{r}
# 2
print(svmfit$index)
```

### Compute the linear coefficients $w$ and $b$.  

Compute the coefficients $w=(w_1, w_2, ...)^\top$ and $b$ can be computed according to the theory by using the values 

+ **svm$coefs**: The corresponding coefficients times the training labels. 

+ **svm$index**: The index of the resulting support vectors in the data matrix. 

+ **svm$rho**: The negative intercept.

... for more details see the Section Values from R Dcoumentation of **svm {e1071}**.  

Save the produced values of the coefficients $w=(w_1, w_2, ...)^\top$ as **w_est** and those of $b$ as **b_est** .

```{r}
X <- cbind(dat$X1,dat$X2)
w_est <- drop(t(svmfit$coefs)%*%X[svmfit$index,])
print(w_est)
b_est <- svmfit$rho
print(b_est)
```

## Make predictions (Optional task)  

This is an optional tast; feel free to skip it

### Recall: How to create a grid of points 

As our feature space is 2 dimensional, it is practically feasible to create a grid of (input) points $x$ that covers the whole (input) domain on a fairly fine grid of points.   

The following given R function returns a grid of points. (After the computer practical, feel free to check the routines used in this function.)     

```{r}
make.grid = function(x, n = 75) {
  grange = apply(x, 2, range)
  x1 = seq(from = grange[1,1], to = grange[2,1], length = n)
  x2 = seq(from = grange[1,2], to = grange[2,2], length = n)
  expand.grid(X1 = x1, X2 = x2)
}
```

To test is, run the following, and understand that the grid goes through the 1st coordinate first, holding the 2nd coordinate fixed.  

```{r}
# generate the grid of points
test_xgrid = make.grid(x, n = 10)
# print the grid of points
test_xgrid[1:10,]
# plot the grid of points
plot(test_xgrid,
     type = "p", 
     pch = 16)
```

###  Do the predictions.  

1. Generate a grid of 50-by-50 (input) points x by using the provided function "make.grid" and save it in the object named "xnew".

2. Compute the predictions for each of the point in "xnew", by using the function "predict()" from "predict.svm {e1071}"

3. Plot the grid of points by using the R function "print{base}" with: 
  
  + pch=20  
+ cex=0.5
+ col: blue for prediction $+1$ and red for predisction $-1$
  
  4. In the same plot, superpose points representing the training data in "dat" by using the function "points{base}". Adjust the color to represent different labels. Use "pch = 19"      

5. In the same plot, superpose the points representing the support points by using the function "points{base}". Adjust the color to represent different labels. Use "pch = 5", "cex = 2".    

6. In the same plot, superpose

1. the line of the decision hyperplane by using "abline{base}" with "lty = 1" . 

2. the line of the two hyperplanes specifying the upper and lower margin by using "abline{base}" with "lty = 2" . 

```{r}
# 1
x_new <- make.grid( x )
# 2
y_new <- predict( svmfit, x_new )
# 3 
plot(x_new, 
     col = c("red","blue")[as.numeric(y_new)],
     pch=20,
     cex=0.5)
# 4
points(dat$X1, dat$X2, 
       col = as.numeric(dat$y), 
       pch = 19)  
# 5 
points(dat$X1[svmfit$index], 
       dat$X2[svmfit$index], 
       pch = 5, 
       cex = 2)
# 6 
# I compute the coefficient estimates 
w_est <- drop(t(svmfit$coefs)%*%x[svmfit$index,])
b_est <- svmfit$rho
# draw the hyperplanes
abline( b_est / w_est[2], -w_est[1] / w_est[2], lty = 1)
abline( (b_est-1) / w_est[2], -w_est[1] / w_est[2], lty = 2)
abline( (b_est+1) / w_est[2], -w_est[1] / w_est[2], lty = 2)
```

##  Inversigate the cost argument  

Re-fit the SVM classifier with the R function **svm{e1071}** with argument **cost=0.1** (aka smaller than before).  

Save the output returned by R function **svm()** as **svmfit**.  

Plot the separator by using the function **plot()** from **plot.svm {e1071}**    

Print the support vectors saved in **svmfit$index**.  

What do you observe?  

```{r}
svmfit <- svm(y ~ ., 
              data = dat, 
              kernel = "linear", 
              cost = 0.1, 
              scale = FALSE)
plot(svmfit, dat)
svmfit$index

# Now that a smaller value of the cost parameter is being used. 
# We obtain a larger number of support vectors, because the margin is now wider. 
# Unfortunately, the svm() function does not explicitly output the coefficients of 
# the linear decision boundary obtained when the support vector classifier is fit, 
# nor does it output the width of the margin.

```

## Cross validation for tuning and model comparison  

We investigate how to decide the value of cost **C** in an automatic manner.    

Check R documentation  of **tune{e1071}** by using **?tune** which will be used to perform cross-validation.

By default, **tune()** performs ten-fold cross-validation on a set of models of interest.  

###  Choose a preferable value for the argument **cost**. 

First we compare different parametrizations of  svm clascifier each of them with different **cost** value.  In particular the values 0.001, 0.01, 0.1, 1, 5, 10, and 100.  

Use the function **tune()** with the arguments:  

+ **METHOD=svm** for using the svm methodology  

+ **kernel = "linear"** for the link "y~1 +x1 +x2" to be linear wrt the regressors.     

+ **ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100))** to test over a range of cost values

Save the output in the object **tune.out **.

```{r}
set.seed(1) # for consistency reasons
tune.out <- tune(METHOD=svm, 
                 y ~ ., data = dat, 
                 kernel = "linear", 
                 ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100))
                 )
```
###  Print the summary

Use function **summary()** to print the results.    

Which value is the preferable one, according to the error measure of comparison?  

Double check with **tune.out$best.model** .

```{r}
summary(tune.out)
#
#We see that cost = 0.1 results in the lowest cross-validation error rate.
```

```{r}
bestmod <- tune.out$best.model
summary(bestmod) 
```

###  Make predictions 

The R function **predict()** from **predict.svm {e1071}**  can be used to predict the class label on a set of test observations at any given value of the cost parameter. 

Set **bestmod <- tune.out$best.model**

```{r}
bestmod <- tune.out$best.model
```

Consider the following new data set.  

```{r}
xtest <- matrix(rnorm(20 * 2), ncol = 2)
ytest <- sample(c(-1, 1), 20, rep = TRUE)
xtest[ytest == 1, ] <- xtest[ytest == 1, ] + 1
testdat <- data.frame(xtest, y = as.factor(ytest))
```

1. Predict the class labels of these test observations. 
    
    Use the best model obtained through cross-validation, aka **bestmod** in order to make predictions.  

```{r}
ypred <- predict(bestmod, testdat)
```

2. Compute the  SVM confusion matrix (miss-classification table)

    Use the R function **table()**.  
    
How many correct classifications?  

```{r}
table(predict = ypred, truth = testdat$y)
```

3. Compute how many correct classifications I can get with **cost = 0.01**.  
    
    Use R function **svm** ; then **predict** ; then **table**.   


```{r}
svmfit <- svm(y ~ ., data = dat, kernel = "linear", 
    cost = .01, scale = FALSE)
ypred <- predict(svmfit, testdat)
table(predict = ypred, truth = testdat$y)
```

---

# Soft SVM with non-linear kernels.  

In order to fit an SVM using a non-linear kernel, we once again use the **svm()** function. 

However, now we use a different value of the parameter `kernel`.  

Function **svm{e1071}** can use a number of different kernel functions that can be found in the R Documentation (check ?svm, Argument kernel) 

## SVM with polynomial Kernel  

Use the following dataset.  

```{r}
set.seed(1)
N = 1000
xx = rnorm(N)
yy = 4 * xx^2 + 1 + rnorm(N)
class = sample(N, N/2)
yy[class] = yy[class] + 6
yy[-class] = yy[-class] - 6
x <- cbind(xx,yy)
plot(x[class,1], x[class,2], col = "red", xlab = "X", ylab = "Y", ylim = c(-6, 30)) +
points(x[-class,1], x[-class,2], col = "blue")
#
y = rep(-1, N)
y[class] = 1
data = data.frame(x, y = as.factor(y))
train = sample(N, N/2)
data_train = data[train, ]
data_eval = data[-train, ]
```




###  Fit Soft SVM with polynomial kernel.  

To fit an SVM with a polynomial kernel we use **kernel = "polynomial"** (check in ?svm).  

Have a look at the Section values in the R Documentation of **svm {e1071}	**

The polynomial kernel is:  

\[
(\gamma u^\top v+c_0)^d
\]

Fit the SVM classifier by using a polynomial kernel of degree $d=2$. In particular, use arguments  

+ **kernel = "polynomial"**, for the polynomial 

+ **degree=2**, for $d$ 

+ **gamma=2**, for $\gamma$ 

+ **coef0=0**, for $c_0$      

+ **cost = 1**  

```{r}
svmfit <- svm(y ~ ., 
              data = data_train, 
              kernel = "polynomial",  
              degree = 2, 
              gamma = 1,
              coef0 = 0,
              cost = 1, 
              scale = FALSE)
```

###  Plot the separator

Plot the result by using the function **plot** from **plot.svm {e1071}**

```{r}
plot(svmfit, data_train)
```

###  Perform Cross Validation to tune  

Choose appropriate values for the parameters of the polynomial kernel in the following ranges  

+ **cost = c(0.01, 0.05, 0.1)** 

+ **gamma = c(0.5,1,2.0)**  

+ **coef0 = c(0,1,2)**  

+ **degree = c(1,2,3)**   

Use the function **tune()** from **tune {e1071}	**.  

Print the results with function **summary()**.  

Which parametrization is preferable?  

```{r}
set.seed(1)
tune.out <- tune(svm, y ~ ., data = data_train, 
    kernel = "polynomial", 
    ranges = list(
      cost = c(0.01, 0.05, 0.1),
      gamma = c(0.5,1,2.0),
      coef0 = c(0,1,2),
      degree = c(1,2,3)
    )
  )
summary(tune.out)
```




## SVM with radial Kernel   

We use the following training data set.  

```{r}
x <- matrix(rnorm(200*2), ncol = 2)
x[1:100,] <- x[1:100,] + 2.5
x[101:150,] <- x[101:150,] - 2.5
y <- c(rep(1,150), rep(2,50))
dat <- data.frame(x,y=as.factor(y))
plot(x, col = y + 1)
```

###  Fit Soft SVM with  radial Kernel  

To fit an SVM with a radial kernel we use kernel = "radial" (check by ?svm).

In Particular the radial kernel is coded as:  

\[
\exp(-\gamma |v-v'|_2^2)
\]

So we can use the arguments  

+ **gamma**, for $\gamma$ 

Fit the SVM classifier by using a radial kernel with parameter $\gamma=2$.   

```{r}
svmfit <- svm( factor(y) ~ ., 
            data = dat, 
            scale = FALSE, 
            kernel = "radial", 
            gamma = 1/2,
            cost = 10)
```

###  Plot the separator

Plot the result by using the function **plot** from **plot.svm {e1071}**

```{r}
plot(svmfit, dat)
```


---  
